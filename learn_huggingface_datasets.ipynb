{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "dataset = load_dataset(\"ChristophSchuhmann/MS_COCO_2017_URL_TEXT\")\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# 定义图像尺寸\n",
    "img_size = 224  # 或适合您模型的图像尺寸\n",
    "\n",
    "# 定义设备（GPU或CPU）\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['URL', 'TEXT'],\n",
       "    num_rows: 591753\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset['train']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591753"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'URL': 'http://images.cocodataset.org/train2017/000000391895.jpg',\n",
       " 'TEXT': 'A man with a red helmet on a small moped on a dirt road. '}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'URL': ['http://images.cocodataset.org/train2017/000000391895.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000391895.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000391895.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000391895.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000391895.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000522418.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000522418.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000522418.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000522418.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000522418.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000184613.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000184613.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000184613.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000184613.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000184613.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000318219.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000318219.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000318219.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000318219.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000318219.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000554625.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000554625.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000554625.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000554625.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000554625.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000574769.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000574769.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000574769.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000574769.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000574769.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000060623.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000060623.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000060623.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000060623.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000060623.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000309022.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000309022.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000309022.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000309022.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000309022.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000005802.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000005802.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000005802.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000005802.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000005802.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000222564.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000222564.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000222564.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000222564.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000222564.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000118113.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000118113.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000118113.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000118113.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000118113.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000193271.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000193271.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000193271.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000193271.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000193271.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000224736.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000224736.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000224736.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000224736.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000224736.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000483108.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000483108.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000483108.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000483108.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000483108.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000403013.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000403013.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000403013.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000403013.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000403013.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000374628.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000374628.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000374628.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000374628.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000374628.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000328757.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000328757.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000328757.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000328757.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000328757.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000384213.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000384213.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000384213.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000384213.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000384213.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000293802.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000293802.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000293802.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000293802.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000293802.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000086408.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000086408.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000086408.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000086408.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000086408.jpg'],\n",
       " 'TEXT': ['A man with a red helmet on a small moped on a dirt road. ',\n",
       "  'Man riding a motor bike on a dirt road on the countryside.',\n",
       "  'A man riding on the back of a motorcycle.',\n",
       "  'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ',\n",
       "  'A man in a red shirt and a red hat is on a motorcycle on a hill side.',\n",
       "  'A woman wearing a net on her head cutting a cake. ',\n",
       "  'A woman cutting a large white sheet cake.',\n",
       "  'A woman wearing a hair net cutting a large sheet cake.',\n",
       "  'there is a woman that is cutting a white cake',\n",
       "  \"A woman marking a cake with the back of a chef's knife. \",\n",
       "  'A child holding a flowered umbrella and petting a yak.',\n",
       "  'A young man holding an umbrella next to a herd of cattle.',\n",
       "  'a young boy barefoot holding an umbrella touching the horn of a cow',\n",
       "  'A young boy with an umbrella who is touching the horn of a cow.',\n",
       "  'A boy holding an umbrella while standing next to livestock.',\n",
       "  'A young boy standing in front of a computer keyboard.',\n",
       "  'a little boy wearing headphones and looking at a computer monitor',\n",
       "  'He is listening intently to the computer at school.',\n",
       "  'A young boy stares up at the computer monitor.',\n",
       "  'a young kid with head phones on using a computer ',\n",
       "  'a boy wearing headphones using one computer in a long row of computers',\n",
       "  'A little boy with earphones on listening to something.',\n",
       "  'A group of people sitting at desk using computers.',\n",
       "  'Children sitting at computer stations on a long table.',\n",
       "  'A small child wearing headphones plays on the computer.',\n",
       "  'A woman in a room with a cat.',\n",
       "  'A girl smiles as she holds a cat and wears a brightly colored skirt.',\n",
       "  'a woman is holding a cat in her kitchen',\n",
       "  'A woman is working in a kitchen carrying a soft toy.',\n",
       "  'A woman is holding a cat in her kitchen.',\n",
       "  'A young girl inhales with the intent of blowing out a candle. ',\n",
       "  'A young girl is preparing to blow out her candle.',\n",
       "  'A kid is to blow out the single candle in a bowl of birthday goodness. ',\n",
       "  'Girl blowing out the candle on an ice-cream ',\n",
       "  'A little girl is getting ready to blow out a candle on a small dessert.',\n",
       "  'A commercial stainless kitchen with a pot of food cooking. ',\n",
       "  'Some food sits in a pot in a kitchen. ',\n",
       "  'A kitchen has all stainless steel appliances and counters.',\n",
       "  'a kitchen with a sink and many cooking machines and a pot of food',\n",
       "  'Food cooks in a pot on a stove in a kitchen.',\n",
       "  'Two men wearing aprons working in a commercial-style kitchen.',\n",
       "  'Chefs preparing food in a professional metallic style kitchen.',\n",
       "  'Two people standing around in a large kitchen.',\n",
       "  'A commercial kitchen with two men working to prepare several plates.',\n",
       "  'two men in white shirts in a large steel kitchen',\n",
       "  'Two chefs in a restaurant kitchen preparing food. ',\n",
       "  'Two cooks are cooking the food someone ordered at this restaurant',\n",
       "  'The chef is cooking with pans on the stove next to an oven. ',\n",
       "  'Two men that are standing in a kitchen.',\n",
       "  'Two cooks are near the stove in a stainless steel kitchen.',\n",
       "  'this is a very dark picture of a room with a shelf',\n",
       "  'a cluttered room with a table and shelf on the wall.',\n",
       "  'A view of a messy room, with shelves on the wall.',\n",
       "  'A dark and cluttered storage area with wood walls.',\n",
       "  'A dim lit room consisting of many objects put together. ',\n",
       "  'A kitchen filled with black appliances and lots of counter top space.',\n",
       "  'some brown cabinets a black oven a tea kettle and a microwave',\n",
       "  'A small kitchen with glass and wooden cabinets.',\n",
       "  'A modern style kitchen filled with may different items.',\n",
       "  'A kitchen with wooden cabinets and black appliances.',\n",
       "  'A professional kitchen filled with sinks and appliances.',\n",
       "  'A kitchen area with toilet and various cleaning appliances.',\n",
       "  'A commercial dish washing station with a toilet in it.',\n",
       "  'A toilet and mop bucket in a kitchen.',\n",
       "  'A cluttered room with a sink, a toilet and in industrial mop bucket.',\n",
       "  'A man on a bicycle riding next to a train',\n",
       "  'A person is riding a bicycle but there is a train in the background.',\n",
       "  'a red and white train and a man riding a bicycle',\n",
       "  'a guy that is riding his bike next to a train',\n",
       "  'A man riding a bike past a train traveling along tracks.',\n",
       "  'A narrow kitchen filled with appliances and cooking utensils.',\n",
       "  'A galley kitchen with cabinets and appliances on both sides',\n",
       "  'A hallway leading into a white kitchen with appliances.',\n",
       "  'Doorway view of a kitchen with a sink, stove, refrigerator and pantry.',\n",
       "  'The pantry door of the small kitchen is closed.',\n",
       "  'A kitchen with wood floors and lots of furniture.',\n",
       "  'A beautiful, open kitchen and dining room area features an island in the center and wood cabinets and large windows.',\n",
       "  'A kitchen made of mostly wood with a small desk with a laptop.',\n",
       "  'A very spacious room with a kitchen and dining area.',\n",
       "  'A full view of an open kitchen and dining area.',\n",
       "  'A woman eating vegetables in front of a stove.',\n",
       "  'A woman forks vegetables out of a bowl into her mouth. ',\n",
       "  'Woman eating an assortment of mixed vegetables in a bowl.',\n",
       "  'A young woman standing in a kitchen eats a plate of vegetables.',\n",
       "  'A woman eating fresh vegetables from a bowl.',\n",
       "  'A kitchen is shown with a variety of items on the counters.',\n",
       "  'A kitchen has the windows open and plaid curtains.',\n",
       "  'A kitchen with two windows and two metal sinks.',\n",
       "  'An older kitchen with cluttered counter tops but empty sink.',\n",
       "  'Glasses and bottles are placed near a kitchen sink.',\n",
       "  'A boy performing a kickflip on his skateboard on a city street.',\n",
       "  'A man is doing a trick on a skateboard',\n",
       "  'A guy jumps in the air with his skateboard beneath him.',\n",
       "  'Man in all black doing a trick on his skateboard.',\n",
       "  'A skateboarder flipping his board on a street.',\n",
       "  'A kitchen with a stove, microwave and refrigerator.',\n",
       "  'A refrigerator, oven and microwave sitting in a kitchen.',\n",
       "  'The kitchenette uses  small space to great efficiency.',\n",
       "  'an image of a kitchen setting with black appliances',\n",
       "  'A kitchen with cabinets, a stove, microwave and refrigerator.']}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[:100]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_datadict(dct, start_idx, end_idx):\n",
    "    slice_dict = {}\n",
    "    keys = list(dct.keys())\n",
    "    for key in keys:\n",
    "        slice_dict[key] = dct[key][start_idx:end_idx]\n",
    "    return slice_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch = slice_datadict(example,0,5)\n",
    "type(minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'URL': ['http://images.cocodataset.org/train2017/000000391895.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000391895.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000391895.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000391895.jpg',\n",
       "  'http://images.cocodataset.org/train2017/000000391895.jpg'],\n",
       " 'TEXT': ['A man with a red helmet on a small moped on a dirt road. ',\n",
       "  'Man riding a motor bike on a dirt road on the countryside.',\n",
       "  'A man riding on the back of a motorcycle.',\n",
       "  'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ',\n",
       "  'A man in a red shirt and a red hat is on a motorcycle on a hill side.']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(minibatch, tokenizer, img_size, device):\n",
    "    augmented_imgs = []\n",
    "    captions = []\n",
    "    value_list = list(minibatch.values())\n",
    "    url_list = value_list[0]\n",
    "    cap_list = value_list[1]\n",
    "    assert len(url_list) == len(cap_list)\n",
    "\n",
    "    for url,cap in zip(url_list,cap_list):\n",
    "        print(f\"url: {url}\")\n",
    "        print(f\"caption: {cap}\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            img_data = response.content\n",
    "        else:\n",
    "            print(f\"Failed to fetch image from URL. Status code: {response.status_code}\")\n",
    "            continue \n",
    "        # img_data = response.content\n",
    "        img = cv2.imdecode(np.frombuffer(img_data, np.uint8), -1)\n",
    "        resize_shape = (img_size, img_size)\n",
    "        img = cv2.resize(img, resize_shape, interpolation=cv2.INTER_LINEAR)\n",
    "        img = np.float32(img) / 255\n",
    "        img = torch.tensor(img)\n",
    "        img = img.permute(2, 1, 0)  # [w, h, c] -> [c, h, w]\n",
    "        transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(int(1.25 * img_size)),  # image_size + 1/4 * image_size\n",
    "            torchvision.transforms.RandomResizedCrop(resize_shape, scale=(0.8, 1.0)),\n",
    "            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # zero mean, unit std\n",
    "        ])\n",
    "        img = transforms(img)\n",
    "        augmented_imgs.append(img)\n",
    "\n",
    "        caption_tokens_dict = tokenizer(cap, return_tensors='pt', padding=True, truncation=True)\n",
    "        captions.append(caption_tokens_dict)\n",
    "    \n",
    "    return augmented_imgs, captions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_(minibatch, img_size):\n",
    "    \"\"\"process the url\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    minibatch: Dict\n",
    "        key: ['URL','text']\n",
    "        value: list[URL],list[text]\n",
    "\n",
    "    img_size: int\n",
    "        the size of image\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    augmented_imgs: List\n",
    "        length of augmented_imgs: batch\n",
    "    captions: List\n",
    "        length of caption: batch\n",
    "    \"\"\"\n",
    "    value_list = list(minibatch.values())\n",
    "    url_list = value_list[0]\n",
    "    captions = value_list[1]\n",
    "    augmented_imgs = []\n",
    "    #processing \n",
    "    for url,cap in zip(url_list,captions):\n",
    "        print(f\"processing url: {url}\")\n",
    "        # print(f\"caption: {cap}\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            img_data = response.content\n",
    "        else:\n",
    "            print(f\"Failed to fetch image from URL. Status code: {response.status_code}\")\n",
    "            continue \n",
    "        # img_data = response.content\n",
    "        img = cv2.imdecode(np.frombuffer(img_data, np.uint8), -1)\n",
    "        resize_shape = (img_size, img_size)\n",
    "        img = cv2.resize(img, resize_shape, interpolation=cv2.INTER_LINEAR)\n",
    "        img = np.float32(img) / 255\n",
    "        img = torch.tensor(img)\n",
    "        img = img.permute(2, 1, 0)  # [w, h, c] -> [c, h, w]\n",
    "        transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(int(1.25 * img_size)),  # image_size + 1/4 * image_size\n",
    "            torchvision.transforms.RandomResizedCrop(resize_shape, scale=(0.8, 1.0)),\n",
    "            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # zero mean, unit std\n",
    "        ])\n",
    "        img = transforms(img)\n",
    "        augmented_imgs.append(img)\n",
    "        \n",
    "    return augmented_imgs, captions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing url: http://images.cocodataset.org/train2017/000000391895.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000391895.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000391895.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000391895.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000391895.jpg\n"
     ]
    }
   ],
   "source": [
    "augmented_imgs, captions = process_batch_(minibatch,img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 1.0000,  1.0000,  1.0000,  ..., -0.2563, -0.1477, -0.0945],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.0134, -0.0268, -0.0477],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.1766,  0.0571, -0.0625],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.7622, -0.6470, -0.5478],\n",
       "          [ 0.9999,  0.9979,  0.9964,  ..., -0.7698, -0.6579, -0.5655],\n",
       "          [ 0.9949,  0.9908,  0.9906,  ..., -0.7948, -0.7225, -0.6816]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.0276,  0.2057,  0.2529],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.2291,  0.2554,  0.2250],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.4288,  0.2953,  0.1307],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.4311, -0.4137, -0.3255],\n",
       "          [ 0.9999,  0.9979,  0.9964,  ..., -0.4335, -0.3547, -0.3042],\n",
       "          [ 0.9973,  0.9923,  0.9906,  ..., -0.5055, -0.4235, -0.3838]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.2502,  0.3998,  0.4102],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.3963,  0.4266,  0.3768],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.5224,  0.4670,  0.3655],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.4281, -0.3203, -0.2662],\n",
       "          [ 0.9999,  0.9979,  0.9964,  ..., -0.4060, -0.3176, -0.2870],\n",
       "          [ 0.9973,  0.9923,  0.9906,  ..., -0.4539, -0.3919, -0.3889]]]),\n",
       " tensor([[[ 1.0000,  1.0000,  1.0000,  ..., -0.2127, -0.2847, -0.3212],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.0087, -0.1439, -0.2296],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.1377, -0.1490, -0.1800],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.8442, -0.8149, -0.8313],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.8055, -0.8317, -0.8381],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.8383, -0.8378, -0.8668]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.1475,  0.0498, -0.0245],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.3606,  0.1195,  0.0548],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.1666,  0.1280,  0.1119],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.6957, -0.6500, -0.6520],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.5885, -0.6196, -0.6352],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.5108, -0.4848, -0.5481]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.3539,  0.2557,  0.1437],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.5290,  0.3012,  0.2366],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.3766,  0.3207,  0.2981],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.6619, -0.6413, -0.5920],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.5409, -0.5820, -0.5647],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.5316, -0.5004, -0.5255]]]),\n",
       " tensor([[[ 1.0000,  1.0000,  1.0000,  ..., -0.1850, -0.1763, -0.2184],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.1516, -0.1345, -0.1718],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.0746,  0.0051, -0.0779],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.6429, -0.6538, -0.7691],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.7687, -0.8308, -0.8651],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.7702, -0.7578, -0.7067]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.0894,  0.1301,  0.0984],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.1583,  0.1672,  0.1122],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.2269,  0.2933,  0.1996],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.2584, -0.2643, -0.4622],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.3510, -0.5099, -0.6364],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.3524, -0.3924, -0.3996]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.3043,  0.3520,  0.3109],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.3694,  0.3884,  0.3367],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.4406,  0.4926,  0.4059],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.3373, -0.3251, -0.4851],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.4009, -0.5058, -0.5932],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.3834, -0.3916, -0.3908]]]),\n",
       " tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  0.3203,  0.7006,  0.5695],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.5715,  0.6739,  0.5685],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.4654,  0.3237,  0.4257],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.4645, -0.5497, -0.7393],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.3801, -0.6177, -0.7487],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.4617, -0.6418, -0.6991]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.4966,  0.8004,  0.6633],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.6048,  0.7377,  0.6458],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.5109,  0.3848,  0.5349],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.1060, -0.2274, -0.3616],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.0423, -0.3623, -0.3504],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.1295, -0.2494, -0.2106]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.6416,  0.8533,  0.6885],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.7069,  0.8030,  0.7133],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.6185,  0.5234,  0.6978],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.0511, -0.1005, -0.2742],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.0098, -0.2474, -0.2616],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.0594, -0.1509, -0.1088]]]),\n",
       " tensor([[[ 1.0000,  1.0000,  1.0000,  ..., -0.0287,  0.0551, -0.0520],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.1092, -0.0568, -0.1263],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.1988, -0.1944, -0.2111],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.6336, -0.6639, -0.7763],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.7735, -0.8369, -0.8661],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.7640, -0.7576, -0.7045]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.2663,  0.3401,  0.2398],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.1858,  0.2553,  0.2153],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.1221,  0.1169,  0.0797],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.2411, -0.2773, -0.4735],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.3656, -0.5215, -0.6401],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.3467, -0.3950, -0.3975]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.4790,  0.5307,  0.4327],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.3897,  0.4544,  0.3999],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.3097,  0.3104,  0.2924],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.3249, -0.3342, -0.4942],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.4140, -0.5126, -0.5952],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ..., -0.3763, -0.3926, -0.3891]]])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A man with a red helmet on a small moped on a dirt road. ',\n",
       " 'Man riding a motor bike on a dirt road on the countryside.',\n",
       " 'A man riding on the back of a motorcycle.',\n",
       " 'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ',\n",
       " 'A man in a red shirt and a red hat is on a motorcycle on a hill side.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img2cap(batch_size,dataset,tokenizer,img_size, device):\n",
    "    \"\"\"load the image-caption dataset and return torch.Tensor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size: int\n",
    "\n",
    "    dataset: DataDict\n",
    "\n",
    "    tokenizer: T5\n",
    "\n",
    "    device: string -- torch.device\n",
    "        cuda or cpu\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img_tenosr: torch.Tensor\n",
    "    caption_tenosr: torch.Tensor\n",
    "\n",
    "    \"\"\"\n",
    "    img_list = []\n",
    "    caption_list = []\n",
    "    n = len(list(dataset.values())[0])\n",
    "    for i in range(0, n, batch_size):\n",
    "        minibatch = slice_datadict(dataset, i, i+batch_size)\n",
    "        augmented_imgs, captions = process_batch_(minibatch, img_size)\n",
    "        img_list.extend(augmented_imgs)\n",
    "        caption_list.extend(captions)\n",
    "        print(\"-----------------------\")\n",
    "    \n",
    "    img_tensor = torch.stack(img_list, dim=0).to(device)\n",
    "    # caption = tokenizer(caption_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    # caption = {key: val.to(device) for key, val in caption.items()}\n",
    "    caption = {\n",
    "        key: val.to(device) if isinstance(val, torch.Tensor) else val\n",
    "        for key, val in tokenizer(caption_list, padding=True, truncation=True, return_tensors=\"pt\").items()\n",
    "    }\n",
    "    \n",
    "    return img_tensor,caption \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing url: http://images.cocodataset.org/train2017/000000391895.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000391895.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000391895.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000391895.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000391895.jpg\n",
      "-----------------------\n",
      "processing url: http://images.cocodataset.org/train2017/000000522418.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000522418.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000522418.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000522418.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000522418.jpg\n",
      "-----------------------\n",
      "processing url: http://images.cocodataset.org/train2017/000000184613.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000184613.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000184613.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000184613.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000184613.jpg\n",
      "-----------------------\n",
      "processing url: http://images.cocodataset.org/train2017/000000318219.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000318219.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000318219.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000318219.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000318219.jpg\n",
      "-----------------------\n",
      "processing url: http://images.cocodataset.org/train2017/000000554625.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000554625.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000554625.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000554625.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000554625.jpg\n",
      "-----------------------\n",
      "processing url: http://images.cocodataset.org/train2017/000000574769.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000574769.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000574769.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000574769.jpg\n",
      "processing url: http://images.cocodataset.org/train2017/000000574769.jpg\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "example = dataset[:30]\n",
    "img, caption = load_img2cap(\n",
    "    batch_size=5,\n",
    "    dataset=example,\n",
    "    tokenizer=tokenizer,\n",
    "    img_size=img_size,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 3, 224, 224])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  71,  388,   28,  ...,    0,    0,    0],\n",
       "         [1140, 7494,    3,  ...,    0,    0,    0],\n",
       "         [  71,  388, 7494,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   3,    9, 2335,  ...,    0,    0,    0],\n",
       "         [  71, 2335,   19,  ...,    0,    0,    0],\n",
       "         [  71, 2335,   19,  ...,    0,    0,    0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 45])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = caption[\"input_ids\"]\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m caption\u001b[39m.\u001b[39;49minput_ids\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'input_ids'"
     ]
    }
   ],
   "source": [
    "caption.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad: <pad>\n"
     ]
    }
   ],
   "source": [
    "print(f\"pad: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_token_id = tokenizer(tokenizer.pad_token, return_tensors='pt', padding=False, truncation=True).input_ids\n",
    "start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_token_id = start_token_id[:, 0]\n",
    "start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_token_id = start_token_id.expand(5, -1).to(device)\n",
    "start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
